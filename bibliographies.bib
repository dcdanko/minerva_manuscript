@inproceedings{Holley2017,
abstract = {The advent of High Throughput Sequencing (HTS) technolo-gies raises a major concern about storage and transmission of data pro-duced by these technologies. In particular, large-scale sequencing projects generate an unprecedented volume of genomic sequences ranging from tens to several thousands of genomes per species. These collections con-tain highly similar and redundant sequences, also known as pan-genomes. The ideal way to represent and transfer pan-genomes is through compres-sion. A number of HTS-specific compression tools have been developed to reduce the storage and communication costs of HTS data, yet none of them is designed to process a pan-genome. In this paper, we present DARRC, a new alignment-free and reference-free compression method. It addresses the problem of pan-genome compression by encoding the sequences of a pan-genome as a guided de Bruijn graph. The novelty of this method is its ability to incrementally update DARRC archives with new genome sequences without full decompression of the archive. DARRC can compress both single-end and paired-end read sequences of any length using all symbols of the IUPAC nucleotide code. On a large P. aeruginosa dataset, our method outperforms all other tested tools. It provides a 30{\%} compression ratio improvement in single-end mode com-pared to the best performing state-of-the-art HTS-specific compression method in our experiments. Availability. DARRC is available at https://github.com/Guillaume Holley/DARRC.},
author = {Holley, Guillaume and Wittler, Roland and Stoye, Jens and Hach, Faraz},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-56970-3_4},
isbn = {9783319569697},
issn = {16113349},
pages = {50--65},
title = {{Dynamic alignment-free and reference-free read compression}},
volume = {10229 LNCS},
year = {2017}
}


@article{Frank2016,
author = {Frank, J A and Pan, Y and Tooming-Klunderud, A and Eijsink, V G H and McHardy, A C and Nederbragt, A J and Pope, P B},
journal = {Scientific Reports},
month = {may},
pages = {25373},
publisher = {The Author(s)},
title = {{Improved metagenome assemblies and taxonomic binning using long-read circular consensus sequence data}},
url = {https://doi.org/10.1038/srep25373 http://10.0.4.14/srep25373 https://www.nature.com/articles/srep25373{\#}supplementary-information},
volume = {6},
year = {2016}
}

@article{Zheng2016,
abstract = {Haplotyping of human chromosomes is a prerequisite for cataloguing the full repertoire of genetic variation. We present a microfluidics-based, linked-read sequencing technology that can phase and haplotype germline and cancer genomes using nanograms of input DNA. This high-throughput platform prepares barcoded libraries for short-read sequencing and computationally reconstructs long-range haplotype and structural variant information. We generate haplotype blocks in a nuclear trio that are concordant with expected inheritance patterns and phase a set of structural variants. We also resolve the structure of the EML4-ALK gene fusion in the NCI-H2228 cancer cell line using phased exome sequencing. Finally, we assign genetic aberrations to specific megabase-scale haplotypes generated from whole-genome sequencing of a primary colorectal adenocarcinoma. This approach resolves haplotype information using up to 100 times less genomic DNA than some methods and enables the accurate detection of structural variants.},
author = {Zheng, Grace X Y and Lau, Billy T and Schnall-Levin, Michael and Jarosz, Mirna and Bell, John M and Hindson, Christopher M and Kyriazopoulou-Panagiotopoulou, Sofia and Masquelier, Donald A and Merrill, Landon and Terry, Jessica M and Mudivarti, Patrice A and Wyatt, Paul W and Bharadwaj, Rajiv and Makarewicz, Anthony J and Li, Yuan and Belgrader, Phillip and Price, Andrew D and Lowe, Adam J and Marks, Patrick and Vurens, Gerard M and Hardenbol, Paul and Montesclaros, Luz and Luo, Melissa and Greenfield, Lawrence and Wong, Alexander and Birch, David E and Short, Steven W and Bjornson, Keith P and Patel, Pranav and Hopmans, Erik S and Wood, Christina and Kaur, Sukhvinder and Lockwood, Glenn K and Stafford, David and Delaney, Joshua P and Wu, Indira and Ordonez, Heather S and Grimes, Susan M and Greer, Stephanie and Lee, Josephine Y and Belhocine, Kamila and Giorda, Kristina M and Heaton, William H and McDermott, Geoffrey P and Bent, Zachary W and Meschi, Francesca and Kondov, Nikola O and Wilson, Ryan and Bernate, Jorge A and Gauby, Shawn and Kindwall, Alex and Bermejo, Clara and Fehr, Adrian N and Chan, Adrian and Saxonov, Serge and Ness, Kevin D and Hindson, Benjamin J and Ji, Hanlee P},
doi = {10.1038/nbt.3432},
edition = {2016/02/01},
issn = {1546-1696},
journal = {Nature biotechnology},
month = {mar},
number = {3},
pages = {303--311},
title = {{Haplotyping germline and cancer genomes with high-throughput linked-read sequencing}},
url = {https://www.ncbi.nlm.nih.gov/pubmed/26829319 https://www.ncbi.nlm.nih.gov/pmc/PMC4786454/},
volume = {34},
year = {2016}
}

    
    
@article{Jain2017,
	Author = {Jain, Miten and Koren, Sergey and Miga, Karen H and Quick, Josh and Rand, Arthur C and Sasani, Thomas A and Tyson, John R and Beggs, Andrew D and Dilthey, Alexander T and Fiddes, Ian T and Malla, Sunir and Marriott, Hannah and Nieto, Tom and O'Grady, Justin and Olsen, Hugh E and Pedersen, Brent S and Rhie, Arang and Richardson, Hollian and Quinlan, Aaron R and Snutch, Terrance P and Tee, Louise and Paten, Benedict and Phillippy, Adam M and Simpson, Jared T and Loman, Nicholas J and Loose, Matthew},
	Date = {2018/01/29/online},
	Date-Added = {2018-10-01 21:58:04 +0000},
	Date-Modified = {2018-10-01 21:58:14 +0000},
	Day = {29},
	Journal = {Nature Biotechnology},
	L3 = {10.1038/nbt.4060; https://www.nature.com/articles/nbt.4060#supplementary-information},
	Month = {01},
	Pages = {338 EP  -},
	Publisher = {The Author(s) SN  -},
	Title = {Nanopore sequencing and assembly of a human genome with ultra-long reads},
	Ty = {JOUR},
	Url = {http://dx.doi.org/10.1038/nbt.4060},
	Volume = {36},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1038/nbt.4060}}


@inproceedings{Orenstein2016,
abstract = {? Springer International Publishing Switzerland 2016.We address the problem of finding a minimum-size set of k-mers that hits L-long sequences. The problem arises in the design of compact hash functions and other data structures for efficient handling of large sequencing datasets. We prove that the problem of hitting a given set of L-long sequences is NP-hard and give a heuristic solution that finds a compact universal k-mer set that hits any set of L-long sequences. The algorithm, called DOCKS (design of compact k-mer sets), works in two phases: (i) finding a minimum-size k-mer set that hits every infinite sequence; (ii) greedily adding k-mers such that together they hit all remaining L-long sequences. We show that DOCKS works well in practice and produces a set of k-mers that is much smaller than a random choice of k-mers. We present results for various values of k and sequence lengths L and by applying them to two bacterial genomes show that universal hitting k-mers improve on minimizers. The software and exemplary sets are freely available at acgt.cs.tau.ac.il/docks/.},
author = {Orenstein, Yaron and Pellow, David and Mar{\c{c}}ais, Guillaume and Shamir, Ron and Kingsford, Carl},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-43681-4_21},
isbn = {9783319436807},
issn = {16113349},
pages = {257--268},
title = {{Compact universal k-mer hitting sets}},
volume = {9838},
year = {2016}
}
@inproceedings{Marcais2017,
abstract = {Motivation: The minimizers scheme is a method for selecting k-mers from sequences. It is used in many bioinformatics software tools to bin comparable sequences or to sample a sequence in a de-terministic fashion at approximately regular intervals, in order to reduce memory consumption and processing time. Although very useful, the minimizers selection procedure has undesirable be-haviors (e.g. too many k-mers are selected when processing certain sequences). Some of these problems were already known to the authors of the minimizers technique, and the natural lexico-graphic ordering of k-mers used by minimizers was recognized as their origin. Many software tools using minimizers employ ad hoc variations of the lexicographic order to alleviate those issues. Results: We provide an in-depth analysis of the effect of k-mer ordering on the performance of the minimizers technique. By using small universal hitting sets (a recently defined concept), we show how to significantly improve the performance of minimizers and avoid some of its worse behav-iors. Based on these results, we encourage bioinformatics software developers to use an ordering based on a universal hitting set or, if not possible, a randomized ordering, rather than the lexico-graphic order. This analysis also settles negatively a conjecture (by Schleimer et al.) on the ex-pected density of minimizers in a random sequence. Availability and Implementation: The software used for this analysis is available on GitHub: https:// github.com/gmarcais/minimizers.git.},
author = {Mar{\c{c}}ais, Guillaume and Pellow, David and Bork, Daniel and Orenstein, Yaron and Shamir, Ron and Kingsford, Carl},
booktitle = {Bioinformatics},
doi = {10.1093/bioinformatics/btx235},
issn = {14602059},
number = {14},
pages = {i110--i117},
title = {{Improving the performance of minimizers and winnowing schemes}},
volume = {33},
year = {2017}
}
@article{Schleimer2003,
abstract = {Digital content is for copying: quotation, revision, plagiarism, and file sharing all create copies. Document fingerprinting is concerned with accurately identifying copying, including small partial copies, within large sets of documents.We introduce the class of local document fingerprinting algorithms, which seems to capture an essential property of any finger-printing technique guaranteed to detect copies. We prove a novel lower bound on the performance of any local algorithm. We also develop winnowing, an efficient local fingerprinting algorithm, and show that winnowing's performance is within 33{\%} of the lower bound. Finally, we also give experimental results on Web data, and report experience with MOSS, a widely-used plagiarism detection service.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Schleimer, Saul and Wilkerson, Daniel S. and Aiken, Alex},
doi = {10.1145/872757.872770},
eprint = {arXiv:1011.1669v3},
isbn = {158113634X},
issn = {10466673},
journal = {Proceedings of the 2003 ACM SIGMOD international conference on on Management of data - SIGMOD '03},
pages = {76--85},
pmid = {24030996},
title = {{Winnowing: Local Algorithms for Document Fingerprinting}},
url = {http://portal.acm.org/citation.cfm?doid=872757.872770},
year = {2003}
}
@article{Droge2017,
abstract = {Shotgun metagenomics of microbial communities reveal information about strains of relevance for applications in medicine, biotechnology and ecology. Recovering their genomes is a crucial but very challenging step due to the complexity of the underlying biological system and technical factors. Microbial communities are heterogeneous, with oftentimes hundreds of present genomes deriving from different species or strains, all at varying abundances and with different degrees of similarity to each other and reference data. We present a versatile probabilistic model for genome recovery and analysis, which aggregates three types of information that are commonly used for genome recovery from metagenomes. As potential applications we showcase metagenome contig classification, genome sample enrichment and genome bin comparisons. The open source implementation MGLEX is available via the Python Package Index and on GitHub and can be embedded into metagenome analysis workflows and programs.},
author = {Dr{\"{o}}ge, Johannes and Sch{\"{o}}nhuth, Alexander and McHardy, Alice C.},
doi = {10.7717/peerj-cs.117},
isbn = {e2626v3},
issn = {2376-5992},
journal = {PeerJ Computer Science},
keywords = {Binning,Metagenomics},
pages = {e117},
title = {{A probabilistic model to recover individual genomes from metagenomes}},
url = {https://peerj.com/articles/cs-117},
volume = {3},
year = {2017}
}
@article{Blei2003,
abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Blei, David M and Ng, Andrew Y and Jordan, Michael I},
doi = {10.1162/jmlr.2003.3.4-5.993},
eprint = {1111.6189v1},
isbn = {9781577352815},
issn = {15324435},
journal = {Journal of Machine Learning Research},
pages = {993--1022},
pmid = {21362469},
title = {{Latent Dirichlet Allocation}},
volume = {3},
year = {2003}
}


@article{McIntyre2017,
	Author = {McIntyre, Alexa B. R. and Ounit, Rachid and Afshinnekoo, Ebrahim and Prill, Robert J. and H{\'e}naff, Elizabeth and Alexander, Noah and Minot, Samuel S. and Danko, David and Foox, Jonathan and Ahsanuddin, Sofia and Tighe, Scott and Hasan, Nur A. and Subramanian, Poorani and Moffat, Kelly and Levy, Shawn and Lonardi, Stefano and Greenfield, Nick and Colwell, Rita R. and Rosen, Gail L. and Mason, Christopher E.},
	Journal = {Genome Biology},
	Number = {1},
	Pages = {182},
	Title = {Comprehensive benchmarking and ensemble approaches for metagenomic classifiers},
	Volume = {18},
	Year = {2017}}

@article{Wood2014,
abstract = {Kraken is an ultrafast and highly accurate program for assigning taxonomic labels to metagenomic DNA sequences. Previous programs designed for this task have been relatively slow and computationally expensive, forcing researchers to use faster abundance estimation programs, which only classify small subsets of metagenomic data. Using exact alignment of k-mers, Kraken achieves classification accuracy comparable to the fastest BLAST program. In its fastest mode, Kraken classifies 100 base pair reads at a rate of over 4.1 million reads per minute, 909 times faster than Megablast and 11 times faster than the abundance estimation program MetaPhlAn. Kraken is available at http://ccb.jhu.edu/software/kraken/.},
archivePrefix = {arXiv},
arxivId = {Figures, S., 2010. Supplementary information. Nature, 1(c), pp.1–7. Available at: http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3006164{\&}tool=pmcentrez{\&}rendertype=abstract.},
author = {Wood, Derrick E and Salzberg, Steven L},
doi = {10.1186/gb-2014-15-3-r46},
eprint = {/www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3006164{\&}tool=pmcentrez{\&}rendertype=abstract.},
isbn = {1465-6914 (Electronic)$\backslash$r1465-6906 (Linking)},
issn = {1465-6906},
journal = {Genome Biology},
mendeley-groups = {Minerva},
number = {3},
pages = {R46},
pmid = {24580807},
primaryClass = {Figures, S., 2010. Supplementary information. Nature, 1(c), pp.1–7. Available at: http:},
title = {{Kraken: ultrafast metagenomic sequence classification using exact alignments}},
url = {http://genomebiology.biomedcentral.com/articles/10.1186/gb-2014-15-3-r46},
volume = {15},
year = {2014}
}

@inproceedings{Ester1996,
abstract = {Clustering techniques are often used for data exploration. In the literature, there are many examples of applications of different clustering methods. The density-based approaches form a separate group within the clustering techniques since they take into account the density of the data. Using the density of data as a similarity measure is practical in many real situations, because clusters of arbitrary shapes can be handled, what is not possible with convectional clustering methods.?? 2009 Elsevier B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {10.1.1.71.1980},
author = {Ester, Martin and Kriegel, Hans-Peter and Sander, Jorg and Xu, Xiaowei},
booktitle = {International Conference on Knowledge Discovery and Information Retrieval},
doi = {10.1016/B978-044452701-1.00067-3},
eprint = {10.1.1.71.1980},
isbn = {9780444527011},
issn = {09758887},
keywords = {Color maps,Core plot,Density-based techniques,Inliers,Natural clusters,Outliers,Reachability plot},
pages = {226--231},
pmid = {15003161},
title = {{A density-based algorithm for discovering clusters in large spatial databases with noise.}},
year = {1996}
}


@article{Greer2017,
	Abstract = {Genome rearrangements are critical oncogenic driver events in many malignancies. However, the identification and resolution of the structure of cancer genomic rearrangements remain challenging even with whole genome sequencing.},
	Author = {Greer, Stephanie U. and Nadauld, Lincoln D. and Lau, Billy T. and Chen, Jiamin and Wood-Bouwens, Christina and Ford, James M. and Kuo, Calvin J. and Ji, Hanlee P.},
	Da = {2017/06/19},
	Date-Added = {2017-08-01 14:21:29 +0000},
	Date-Modified = {2017-08-01 14:21:41 +0000},
	Doi = {10.1186/s13073-017-0447-8},
	Id = {Greer2017},
	Isbn = {1756-994X},
	Journal = {Genome Medicine},
	Number = {1},
	Pages = {57},
	Title = {Linked read sequencing resolves complex genomic rearrangements in gastric cancer metastases},
	Ty = {JOUR},
	Url = {https://doi.org/10.1186/s13073-017-0447-8},
	Volume = {9},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1186/s13073-017-0447-8},
	Bdsk-Url-2 = {http://dx.doi.org/10.1186/s13073-017-0447-8}}


@article{Spies2017,
	Abstract = {In read cloud approaches, microfluidic partitioning of long genomic DNA fragments and barcoding of shorter fragments derived from these fragments retains long-range information in short sequencing reads. This combination of short reads with long-range information represents a powerful alternative to single-molecule long-read sequencing. We develop Genome-wide Reconstruction of Complex Structural Variants (GROC-SVs) for SV detection and assembly from read cloud data and apply this method to Illumina-sequenced 10x Genomics sarcoma and breast cancer data sets. Compared with short-fragment sequencing, GROC-SVs substantially improves the specificity of breakpoint detection at comparable sensitivity. This approach also performs sequence assembly across multiple breakpoints simultaneously, enabling the reconstruction of events exhibiting remarkable complexity. We show that chromothriptic rearrangements occurred before copy number amplifications, and that rates of single-nucleotide variants and SVs are not correlated. Our results support the use of read cloud approaches to advance the characterization of large and complex structural variation.},
	Author = {Spies, Noah and Weng, Ziming and Bishara, Alex and McDaniel, Jennifer and Catoe, David and Zook, Justin M and Salit, Marc and West, Robert B and Batzoglou, Serafim and Sidow, Arend},
	Date = {2017/07/17/online},
	Date-Added = {2017-08-01 14:23:07 +0000},
	Date-Modified = {2017-08-01 14:23:15 +0000},
	Day = {17},
	Isbn = {1548-7105},
	Journal = {Nat Meth},
	L3 = {10.1038/nmeth.4366; http://www.nature.com/nmeth/journal/vaop/ncurrent/abs/nmeth.4366.html#supplementary-information},
	M3 = {Article},
	Month = {07},
	Pages = {915--920},
	Publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	Title = {Genome-wide reconstruction of complex structural variants using read clouds},
	Ty = {JOUR},
	Url = {http://dx.doi.org/10.1038/nmeth.4366},
	Volume = {9},
	Year = {2017},
	Bdsk-Url-1 = {http://dx.doi.org/10.1038/nmeth.4366}}
    
    
   @article{Moss2017,
abstract = {Shotgun short-read sequencing methods facilitate study of the genomic content and strain-level architecture of complex microbial communities. However, existing methodologies do not capture structural differences between closely related co-occurring strains such as those arising from horizontal gene transfer and insertion sequence mobilization. Recent techniques that partition large DNA molecules, then barcode short fragments derived from them, produce short-read sequences containing long-range information. Here, we present a novel application of these short-read barcoding techniques to metagenomic samples, as well as Athena, an assembler that uses these barcodes to produce improved metagenomic assemblies. We apply our approach to longitudinal samples from the gut microbiome of a patient with a hematological malignancy. This patient underwent an intensive regimen of multiple antibiotics, chemotherapeutics and immunosuppressants, resulting in profound disruption of the microbial gut community and eventual domination by Bacteroides caccae. We significantly improve draft completeness over conventional techniques, uncover strains of B. caccae differing in the positions of transposon integration, and find the abundance of individual strains to fluctuate widely over the course of treatment. In addition, we perform RNA sequencing to investigate relative transcription of genes in B. caccae, and find overexpression of antibiotic resistance genes in our de novo assembled draft genome of B. caccae coinciding with both antibiotic administration and the appearance of proximal transposons harboring a putative bacterial promoter region. Our approach produces overall improvements in contiguity of metagenomic assembly and enables assembly of whole classes of genomic elements inaccessible to existing short-read approaches.},
author = {Moss, Eli and Bishara, Alex and Tkachenko, Ekaterina and Kang, Joyce B and Andermann, Tessa M and Wood, Christina and Handy, Christine and Ji, Hanlee and Batzoglou, Serafim and Bhatt, Ami S},
journal = {bioRxiv},
mendeley-groups = {IH Lab Grant,Minerva},
month = {apr},
title = {{De novo assembly of microbial genomes from human gut metagenomes using barcoded short read sequences}},
url = {http://biorxiv.org/content/early/2017/04/07/125211.abstract},
year = {2017}
}
@misc{longrangerurl,
title = {10X Genomics Long Ranger Pipelines: https://support.10xgenomics.com/genome-exome/software/pipelines/latest/advanced/other-pipelines}
}
@misc{McCallum2002,
abstract = {MALLET is a Java-based package for statistical natural language processing, document classification, clustering, topic modeling, information extraction, and other machine learning applications to text.},
author = {McCallum, Andrew Kachites},
booktitle = {Http://Mallet.Cs.Umass.Edu},
mendeley-groups = {Minerva},
title = {{MALLET: A Machine Learning for Language Toolkit}},
url = {http://mallet.cs.umass.edu},
year = {2002}
}
@inproceedings{Jain2017,
abstract = {Emerging single-molecule sequencing technologies from Pa- cific Biosciences and Oxford Nanopore have revived interest in long read mapping algorithms. Alignment-based seed-and-extend methods demonstrate good accuracy, but face limited scalability, while faster alignment- free methods typically trade decreased precision for efficiency. In this paper, we combine a fast approximate read mapping algorithm based on minimizers with a novel MinHash identity estimation technique to achieve both scalability and precision. In contrast to prior methods, we develop a mathematical framework that defines the types of mapping targets we uncover, establish probabilistic estimates of p-value and sensitivity, and demonstrate tolerance for alignment error rates up to 20{\%}. With this framework, our algorithm automatically adapts to different minimum length and identity requirements and provides both positional and identity estimates for each mapping reported. For mapping human PacBio reads to the hg38 reference, our method is 290x faster thanBWA- MEM with a lower memory footprint and recall rate of 96{\%}. We further demonstrate the scalability of our method by mapping noisy PacBio reads (each ≥ 5 kbp in length) to the complete NCBI RefSeq database containing 838 Gbp of sequence and {\textgreater} 60, 000 genomes. Keywords:},
author = {Jain, Chirag and Dilthey, Alexander and Koren, Sergey and Aluru, Srinivas and Phillippy, Adam M.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-56970-3_5},
isbn = {9783319569697},
issn = {16113349},
keywords = {Jaccard,Long read mapping,MinHash,Minimizers,Nanopore,PacBio,Sketching,Winnowing},
pages = {66--81},
title = {{A fast approximate algorithm for mapping long reads to large reference databases}},
volume = {10229 LNCS},
year = {2017}
}



@article{Girvan2002,
abstract = {A number of recent studies have focused on the statistical properties of networked systems such as social networks and the Worldwide Web. Researchers have concentrated particularly on a few properties that seem to be common to many networks: the small-world property, power-law degree distributions, and network transitivity. In this article, we highlight another property that is found in many networks, the property of community structure, in which network nodes are joined together in tightly knit groups, between which there are only looser connections. We propose a method for detecting such communities, built around the idea of using centrality indices to find community boundaries. We test our method on computer-generated and real-world graphs whose community structure is already known and find that the method detects this known structure with high sensitivity and reliability. We also apply the method to two networks whose community structure is not well known--a collaboration network and a food web--and find that it detects significant and informative community divisions in both cases.},
archivePrefix = {arXiv},
arxivId = {cond-mat/0112110},
author = {Girvan, M. and Newman, M. E. J.},
doi = {10.1073/pnas.122653799},
eprint = {0112110},
isbn = {0027-8424},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
number = {12},
pages = {7821--7826},
pmid = {12060727},
primaryClass = {cond-mat},
title = {{Community structure in social and biological networks}},
url = {http://www.pnas.org/cgi/doi/10.1073/pnas.122653799},
volume = {99},
year = {2002}
}

@article{Abbe2016,
abstract = {The stochastic block model (SBM) with two communities, or equivalently the planted bisection model, is a popular model of random graph exhibiting a cluster behaviour. In the symmetric case, the graph has two equally sized clusters and vertices connect with probability {\$}p{\$} within clusters and {\$}q{\$} across clusters. In the past two decades, a large body of literature in statistics and computer science has focused on providing lower-bounds on the scaling of {\$}|p-q|{\$} to ensure exact recovery. In this paper, we identify a sharp threshold phenomenon for exact recovery: if {\$}\backslashalpha=pn/\backslashlog(n){\$} and {\$}\backslashbeta=qn/\backslashlog(n){\$} are constant (with {\$}\backslashalpha{\textgreater}\backslashbeta{\$}), recovering the communities with high probability is possible if {\$}\backslashfrac{\{}\backslashalpha+\backslashbeta{\}}{\{}2{\}} - \backslashsqrt{\{}\backslashalpha \backslashbeta{\}}{\textgreater}1{\$} and impossible if {\$}\backslashfrac{\{}\backslashalpha+\backslashbeta{\}}{\{}2{\}} - \backslashsqrt{\{}\backslashalpha \backslashbeta{\}}{\textless}1{\$}. In particular, this improves the existing bounds. This also sets a new line of sight for efficient clustering algorithms. While maximum likelihood (ML) achieves the optimal threshold (by definition), it is in the worst-case NP-hard. This paper proposes an efficient algorithm based on a semidefinite programming relaxation of ML, which is proved to succeed in recovering the communities close to the threshold, while numerical experiments suggest it may achieve the threshold. An efficient algorithm which succeeds all the way down to the threshold is also obtained using a partial recovery algorithm combined with a local improvement procedure.},
archivePrefix = {arXiv},
arxivId = {1405.3267},
author = {Abbe, Emmanuel and Bandeira, Afonso S. and Hall, Georgina},
doi = {10.1109/TIT.2015.2490670},
eprint = {1405.3267},
isbn = {0018-9448},
issn = {00189448},
journal = {IEEE Transactions on Information Theory},
keywords = {Clustering algorithms,Communities,Detection algorithms,Network theory (graphs),Statistical learning},
number = {1},
pages = {471--487},
title = {{Exact recovery in the stochastic block model}},
volume = {62},
year = {2016}
}

@article{Mossel2014,
abstract = {We consider the problem of reconstructing symmetric block models with two blocks of {\$}n{\$} vertices each and connection probabilities {\$}p{\_}n{\$} and {\$}q{\_}n{\$} for inter- and intra-block edge probabilities respectively. We are interested in two different notions of consistency: strong consistency means the existence of an estimation procedure which recovers the labels of all nodes correctly with probability {\$}1-o(1){\$} as {\$}n \backslashto \backslashinfty{\$}, while weak consistency only asks for the estimation procedure to correctly label {\$}2n - o(n){\$} nodes correctly. We provide necessary and sufficient conditions on {\$}p{\_}n{\$} and {\$}q{\_}n{\$} for both notions of consistency, and we relate these conditions to the structure of the corresponding graphs. In particular, strong consistency is attainable if and only if with high probability every node has the same label as the majority of its neighbors, while weak consistency is attainable if and only if each node has, with high probability, the same label as the majority of its neighbors. We also give efficient algorithms for consistent estimators whenever one exists. Our results hold for arbitrary sequences {\$}p{\_}n{\$} and {\$}q{\_}n{\$}.},
archivePrefix = {arXiv},
arxivId = {1407.1591},
author = {Mossel, Elchanan and Neeman, Joe and Sly, Allan},
eprint = {1407.1591},
isbn = {0001411101},
journal = {arXiv preprint arXiv:1407.1591},
title = {{Consistency Thresholds for Binary Symmetric Block Models}},
url = {http://arxiv.org/abs/1407.1591v1{\%}5Cnhttp://arxiv.org/abs/1407.1591},
year = {2014}
}

@inproceedings{Hajek2016,
abstract = {The binary symmetric stochastic block model deals with a random graph of {\$}n{\$} vertices partitioned into two equal-sized clusters, such that each pair of vertices is connected independently with probability {\$}p{\$} within clusters and {\$}q{\$} across clusters. In the asymptotic regime of {\$}p=a \backslashlog n/n{\$} and {\$}q=b \backslashlog n/n{\$} for fixed {\$}a,b{\$} and {\$}n \backslashto \backslashinfty{\$}, we show that the semidefinite programming relaxation of the maximum likelihood estimator achieves the optimal threshold for exactly recovering the partition from the graph with probability tending to one, resolving a conjecture of Abbe et al. $\backslash$cite{\{}Abbe14{\}}. Furthermore, we show that the semidefinite programming relaxation also achieves the optimal recovery threshold in the planted dense subgraph model containing a single cluster of size proportional to {\$}n{\$}.},
archivePrefix = {arXiv},
arxivId = {1412.6156},
author = {Hajek, Bruce and Wu, Yihong and Xu, Jiaming},
booktitle = {IEEE Transactions on Information Theory},
doi = {10.1109/TIT.2016.2594812},
eprint = {1412.6156},
isbn = {9781467385763},
issn = {00189448},
keywords = {Community detection,Erd's-R{\'{e}}nyi random graph,Semidefinite programming,Stochastic block model},
number = {10},
pages = {5918--5937},
title = {{Achieving Exact Cluster Recovery Threshold via Semidefinite Programming: Extensions}},
volume = {62},
year = {2016}
}

@article{Mason2017,
abstract = {Challenges and biases in preparing, characterizing, and sequencing DNA and RNA can have significant impacts on research in genomics across all kingdoms of life, including experiments in single-cells, RNA profiling, and metagenomics (across multiple genomes). Technical artifacts and contamination can arise at each point of sample manipulation, extraction, sequencing, and analysis. Thus, the measurement and benchmarking of these potential sources of error are of paramount importance as next-generation sequencing (NGS) projects become more global and ubiquitous. Fortunately, a variety of methods, standards, and technologies have recently emerged that improve measurements in genomics and sequencing, from the initial input material to the computational pipelines that process and annotate the data. Here we review current standards and their applications in genomics, including whole genomes, transcriptomes, mixed genomic samples (metagenomes), and the modified bases within each (epigenomes and epitranscriptomes). These standards, tools, and metrics are critical for quantifying the accuracy of NGS methods, which will be essential for robust approaches in clinical genomics and precision medicine. },
address = {Bethesda, MD, USA},
author = {Mason, Christopher E and Afshinnekoo, Ebrahim and Tighe, Scott and Wu, Shixiu and Levy, Shawn},
doi = {10.7171/jbt.17-2801-006},
issn = {1524-0215},
journal = {Journal of Biomolecular Techniques : JBT},
month = {apr},
number = {1},
pages = {8--18},
publisher = {Association of Biomolecular Resource Facilities},
title = {{International Standards for Genomes, Transcriptomes, and Metagenomes}},
url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5359768/},
volume = {28},
year = {2017}
}

